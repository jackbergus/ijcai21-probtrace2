
\subsection{Plan Recognition}
\textit{Plan recognition} predicts an agent's plan based on its observed actions. In such scenarios, it is commonly assumed that the possible activities can be exhaustively enumerated in finite time \cite{RamirezG09}. 

Although  an unexpected context change could make the agent change its plan from the initial $\sigma'$ to $\sigma''$, the already-executed plan $\sigma'$ cannot be retracted. Thus, the  resulting set of actions ${\sigma}$ after the context change  shares intermediate features between $\sigma''$ and $\sigma'$, which differences can be described by plan repairs \cite{FoxGLS06}. Given that the set of all the possible plans can be described using stochastic processes such as HMM \cite{LI2020101974}, the plan recognition problem can be reduced to a sequence analysis as in the biological use case. We can use our pipeline to solve the plan recognition problem and exploit the reduction to the \textit{k}NN problem. %the top-$k$ query to retrieve all the  initial and target plans likely to $\sigma$ by considering both sequence probability and their similarity.


Previous approaches considered the shortest paths towards a goal as the optimal plans \cite{RamirezG10}: we can model this in our pipeline by %this requirement can be met by 
representing a shortest path of interest as a TG $G$ (Fig.\ref{fig:taustar}). Next, given that the posterior probability $\mathbb{P}(g|\nonlogtrace)$ of having $g\in\mathcal{G}$ as a plan's goal is approximated  as $\propto \mathbb{P}(\nonlogtrace|g)\mathbb{P}(g)$, where $\mathbb{P}(g)$ is a probability distribution  over the goals $g$ and $\mathbb{P}(g|\nonlogtrace)$ is  proportional to $s_d(\logtrace,\nonlogtrace)$, we can exploit the Optimal-Ranking by $s=s_d(\logtrace,\nonlogtrace)$ and $p=\mathbb{P}(g)$, and Approximate-Ranking by choosing $(G,\mathbb{P}(g))$ as a weighted TG for the $G$-embedding.