% !TeX root=../main.tex

\section{Introduction}
\label{introduction}

Trace alignment is a prominent problem in AI, where the goal is to detect the trace, from a formal specification of a system,
which best match an observed behaviour. This problem has applications in many areas. For example, in Business Process Modelling, 
one may want to reconstruct the full process from a partially observed trace. Alternatively, if events are not registered immediately,
a log may modify the order of the events; one should be able to detect and correct these deviations.

When the underlying system is vulnerable to uncertainty 


We introduce two ranking strategies. The first one is based on a brute force approach that reuses existing trace aligners such as \cite{DBLP:conf/edoc/AdriansyahDA11,LeoniM17}, where the (optimal) ranking of the top-k alignments is obtained by computing the Levensthein distance between the trace to be aligned and all the model traces and by multiplying each of these distance by the probability of the corresponding model trace. However, even if this approach returns the best trace alignment ranking for a query trace, the alignments must be computed a-new for all the possible traces to be aligned. For models generating a large number of model traces, this would clearly become unfeasible. Therefore, we propose a second strategy that produces an approximate ranking where $x$ and $\mathcal{X}$ are represented as numerical vectors via an embedding $\phi$. {Then, by exploiting ad-hoc data structures,
we can retrieve the neighborhood of $x$ in $\mathcal{X}$ of size $k$  by pre-ordering (\textit{indexing}) $\mathcal{X}$  via a distance between the numerical vectors obtained using $\phi$. Thus, we do not need to analyze the entire space, but just start the search from the top-$1$ alignment. If the embeddings $\phi$ for $\mathcal{X}$ are independent of the query of choice $x$, this would not require to constantly recompute the numeric vector representation for $\mathcal{X}$.
